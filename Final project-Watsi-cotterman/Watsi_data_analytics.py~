
###############################################################################
########## Data Analytics for Maximizing Donations to Watsi ###################
###############################################################################

import os
import re
import numpy as np
import pandas as pd
import urllib2
from datetime import datetime
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
import random as random

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn import cross_validation
from sklearn.cross_validation import cross_val_score
from sklearn import grid_search
from sklearn import metrics

###############################################################################

#folders within folder from which code is run
downloadfolder = "/Watsi_data_downloaded/"
processedfolder = "/Watsi_data_processed/"
resultsfolder = "/Summary_plots_and_tables/"
ppath = os.getcwd()

#file containing main data of interest
towrite = "Data_summary.csv" 
towrite_path = ppath + processedfolder + towrite

###############################################################################


def get_spreadsheet_info():
    """ Extract basic info from the posted google doc.

    Info extracted includes:           
          #Age
          #Region (Latin America vs. Asia vs. Africa)
          #Cost of treatment
          #Date posted
          #Number of days posted
          #Donation rate (dollars per day)
    """

    #use regular expression matching to find desired info
    date_pat = re.compile('\d+/\d+/\d+')
    #profileID_pat = re.compile()
    profile_pat = re.compile(
        '<a target="_blank" href="(https://watsi.org/profile/.*?-.*?)">\d+</a>')
    age_and_loc_pat = re.compile(
        "<td class=\"s\d+\">(\d+)</td><td dir=\"ltr\" class=\"s\d+\">(.*?)<")
    costs_pat = re.compile('\$([0-9,]*\.\d{2})<')
    
    myfile = "watsi_spreadsheet_v2.html" #downloaded on Dec 9, 2013
    file = os.path.join(ppath + downloadfolder + myfile)

    #create dataframe in which to store spreadsheet info
    mydf = pd.DataFrame(index=range(10000), 
        columns=['profile_ID','profile_link', 
                'date_posted', 'date_funded', 'funding_time',
                'patient_age', 'country', 'treat_cost'])
    patient_num = 0
    for line in open(file):
        # split the line into table rows
        rows = re.split("</tr><tr.*?>", line)
        if len(rows) == 1: continue #this line does not contain patient info
        for row in rows: 

            # try to find a profile link, to verify this is a row in the data table
            profile_links = re.findall(profile_pat, row)
            # if we can't find a profile link, skip this row
            if len(profile_links) == 0: 
                continue
            else:
                assert len(profile_links) == 1 
                profile_link = profile_links[0]

            #obtain profile ID
            soup = BeautifulSoup(row)
            a_tag = soup.findAll('a')
            assert len(a_tag)>1
            profile_ID = "{0:0>3}".format(a_tag[1].text)

            dates = re.findall(date_pat, row)
            # skip this row if profile has not yet been funded (funding date missing)
            if len(dates) < 2:
                continue
            #clear up what appears to be data entry mistake (2013 is not a leap year)
            if dates[0]=="2/29/2013":
                dates[0]="2/28/2013"
            if dates[1]=='8/1/20213':
                dates[1]='8/1/2013'
            #correct data typo (there are 2 profiles with ID 133 and none with ID 132)
            if profile_ID=="133" and dates[0]=="2/11/2013":
                profile_ID="132"
            date_posted = datetime.strptime(dates[0], '%m/%d/%Y')
            date_funded = datetime.strptime(dates[1], '%m/%d/%Y')
            #eliminate the profiles created in June of 2012 -- this was before watsi
                #appears to have had any traction 
                #(strange time for funding as a result)
            if date_posted < datetime(2012, 7, 1):
                continue
            funding_time = date_funded - date_posted

            ages_and_locs = re.findall(age_and_loc_pat, row)
            if len(ages_and_locs)<1:
                print "problem with ages_and_locs in profile ",\
                     profile_link , ages_and_locs
                continue
            if len(ages_and_locs[0])<1:
                print "problem with ages_and_locs in profile " ,\
                    profile_link , ages_and_locs
                continue
            #only 1 patient from somaliland (next to Ethiopia)
                #inclusion as separate country leads to misleading graphs/charts
            country = ages_and_locs[0][1]
            if country=="Somaliland": 
                country="Ethiopia"

            treat_cost_string = re.findall(costs_pat, row)[0]
            treat_cost = float(treat_cost_string.replace(",",""))
            if treat_cost>2900: #drop one extreme outlier that throws off results
                treat_cost=None

            #save data
            mydf.profile_ID[patient_num] = profile_ID
            mydf.profile_link[patient_num] = profile_link
            mydf.date_posted[patient_num] = date_posted
            mydf.date_funded[patient_num] = date_funded
            mydf.funding_time[patient_num] = funding_time
            mydf.patient_age[patient_num] = int(ages_and_locs[0][0])
            mydf.country[patient_num] = country
            mydf.treat_cost[patient_num] = treat_cost
            patient_num+=1

    #get rid of empty rows
    mydf = mydf.drop(mydf.index[range(patient_num,mydf.shape[0])])

    print "\nWe have " , mydf.shape[0], " profiles in Watsi's main spreadsheet.\n"   
    
    return mydf


def gender_pronouns(mytext, profile_ID):
    """ Determine patient gender based on keyword search in profile
        If sufficiently ambiguous, look up profile by hand
    """

    male_words = ['he','his','him','man','boy']
    female_words = ['she','her','woman','girl']
    male_count = 0.
    female_count = 0.    
    
    for word in mytext.split(" "):
        for male_word in male_words:
            if word.lower() == male_word:
                male_count +=1
                continue
        for female_word in female_words:
            if word.lower() == female_word:
                female_count +=1
                continue

    if male_count+female_count > 0:
        proportion_male = male_count/ (male_count+female_count)
    else:
        print "Warning:  no gender-specific words for profile " , profile_ID
        proportion_male =  None

    #replace ambiguous gender with hand look-up
        #(for proportion_male between .3 and .7)
    if profile_ID==11:
        proportion_male = 1
    elif profile_ID==89:
        proportion_male = 1
    elif profile_ID==125:
        proportion_male = 0
    elif profile_ID==160:
        proportion_male = 1
    elif profile_ID==225:
        proportion_male = 0
    elif profile_ID==229:
        proportion_male = 1
    elif profile_ID==263:
        proportion_male = 0
    elif profile_ID==313:
        proportion_male = 1
    elif profile_ID==324:
        proportion_male = 1 #adorable!!
    elif profile_ID==482:
        proportion_male = 1 #adorable!!
    elif profile_ID==509:
        proportion_male = 1 #adorable!!
    elif profile_ID==738:
        proportion_male = 1 
    elif profile_ID==459:
        proportion_male = 0 
    elif profile_ID==887:
        proportion_male = 1
    elif profile_ID==889:
        proportion_male = 1
    elif profile_ID==920:
        proportion_male = 1
    elif profile_ID==940:
        proportion_male = 1
    
    #for now, assign gender based on majority of gender-specific pronouns
    #todo: there were a total of 99 profiles with both gender pronouns
        #might want to examine more cases (or make rules involving babies, etc.)
    if proportion_male>=.5:
        male = 1
    elif proportion_male<.5:
        male = 0
    else:
        male = None

    return male


def get_facial_expression(profile_count):
    """Process smile data in csv file
    """

    myfile = "smiles.csv" #classifications done by human inspection
    file = os.path.join(ppath + downloadfolder + myfile)

    #create dataframe in which to store spreadsheet info
    df_smile = pd.DataFrame(index=range(profile_count+200), 
        columns=['profile_ID','smile_scale'])
    
    for line_num, line in enumerate(open(file)):
        df_smile.profile_ID[line_num] = "{0:0>3}".format(line_num+1) #gen profileID
        df_smile.smile_scale[line_num] = float(line.split(",")[0]) #smile score 0 - 1
        #cleaner alternative to change 99s to missing: data.replace(99, np.nan)
        if df_smile.smile_scale[line_num]==99: 
            df_smile.smile_scale[line_num]=None
    print "\nWe have imported smile info for " , \
        line_num, " profiles. (includes missings.)"

    return df_smile


def get_text_and_photo_data(mydf_main, download):
    """ Follow links from main spreadsheet to individual profiles.
        Parse profile data.
    """

    #create dataframe in which to store additional info
    mydf_addon = pd.DataFrame(index=range(mydf_main.shape[0]), 
        columns=['maleness', 'text_length'])

    print "Extract profile text and photo info for " , \
        mydf_main.shape[0], " profiles."
    for row_num in xrange(mydf_main.shape[0]):
 
        #certain profiles are not available
        if row_num!=781 and row_num!=836 and row_num!=968: 

            myfile = "profile_" + str(row_num) + ".html" # file with downloaded page
            file = ppath + downloadfolder + myfile

            #download page info for each profile
            if download==1:
                url_profile = mydf_main.profile_link[row_num]
                response = urllib2.urlopen(url_profile)
                with open(file, 'w') as f: f.write(response.read()) #write to file

            get_text = 0
            mytext = ""
            for line_num, line in enumerate(open(file)):

                begin_pat = re.compile(
                    '</a></div></div><div class="row description"><div class="span2">Description:</div><div')
                begin_text = re.findall(begin_pat, line)
                if len(begin_text)>0:
                    get_text=1

                if get_text==1:
                    mytext = mytext + line

                end_pat = re.compile(
                    '</div></div><div class="row transparency"><div class="span2">Transparency:</div><div')
                end_text = re.findall(end_pat, line)
                if len(end_text)>0:
                    get_text=0

            #clean up text a bit
            start_find = '>Description:</div><div class="span8"><p>'
            start_char = mytext.find(start_find)
            end_char = mytext.find('</div></div><div class="row transparency">')
            if  start_char==-1 or end_char==-1:
                print "\nWarning: could not find text descriptors for row ",\
                     row_num , "\n"
                #print mytext
                continue
            mytext = mytext[start_char+len(start_find):end_char]
            mytext_clean = mytext.replace("<p>"," ")
            mytext_clean = mytext_clean.replace("</p>"," ")
            #print "\n mytext:" , mytext_clean

            #number of characters in text description
            text_length = len(mytext_clean)

            #obtain proportion of gender-specific words that are male 
            proportion_male = gender_pronouns(
                mytext_clean, mydf_main.profile_ID[row_num])
            #print "\n proportion_male: " , proportion_male
        
            #place summary of text in dataframe
            mydf_addon.text_length[row_num] = text_length
            mydf_addon.maleness[row_num] = proportion_male

    #obtain facial expression info
    mydf_photo = get_facial_expression(mydf_main.shape[0])
    #print "\nSample from photo info dataframe:\n" , mydf_photo.ix[:20]

    mydf_expand1 = pd.merge(mydf_main, mydf_addon, 
        left_index=True, right_index=True, how='left')
    mydf_expand2 = pd.merge(mydf_expand1, mydf_photo, on='profile_ID', how='left')  

    return mydf_expand2


def create_age_groups(mydf):
    """ Create age groups for simplifying presentation of summary stats
    """

    #create age_group variable
    age_group_list = []
    for row_num in xrange(mydf.shape[0]):
        #from text descriptions, 
            #it's clear that database lumped 0-yr olds with 1-yr olds
        if mydf.patient_age[row_num]==1:
            age_group="infant (0-1)"
        elif mydf.patient_age[row_num]<6:
            age_group="toddler/preschool (2-5)"
        elif mydf.patient_age[row_num]<12:
            age_group="school-age (6-11)"
        elif mydf.patient_age[row_num]<20:
            age_group="preteen/teen (12-19)"
        elif mydf.patient_age[row_num]<30:
            age_group="twenty-something (20-29)"
        elif mydf.patient_age[row_num]<40:
            age_group="thirty-something (30-39)"
        elif mydf.patient_age[row_num]>=40 and pd.notnull(mydf.patient_age[row_num]):
            age_group="forty plus (40+)"
        else:
            age_group=None

        age_group_list.append(age_group)

    mydf['age_group']=age_group_list

    #alternative (better) method: see "Discretization and Binning" in pandas manual

    return mydf


def create_day_posted(mydf):
    """Create a numeric variable representing day of posting.
        This will be useful for regresions.
    """

    #create a day number variable to use for regresions
    first_day = mydf['date_posted'].min()
    day_num_list = []
    for row_num in xrange(mydf.shape[0]):
        time_elapsed = mydf.date_posted[row_num] - first_day 
        #convert datetime.timedelta to number of days
        if pd.notnull(mydf.date_posted[row_num]):
            day_num = time_elapsed.total_seconds() / (60.*60*24)
        day_num_list.append(day_num)
    mydf.sort(['date_posted'])
    mydf['day_posted'] = day_num_list

    return mydf


def write_data_csv(mydf):
    """ Do some additional data wrangling and then write to csv
    """

    mydf = create_age_groups(mydf)
    mydf = create_day_posted(mydf)
    
    dollars_per_day_list = []
    days_diff_list = []
    region_list = []
    for row_num in xrange(mydf.shape[0]):

        #indicate geographic region (for 18 countries)
        if mydf.country[row_num]=="Kenya" or mydf.country[row_num]=="Tanzania" or \
            mydf.country[row_num]=="Uganda" or mydf.country[row_num]=="Nigeria" or \
            mydf.country[row_num]=="Ghana" or mydf.country[row_num]=="Somaliland" or \
            mydf.country[row_num]=="Mali" or mydf.country[row_num]=="Malawi" or \
            mydf.country[row_num]=="Zambia" or mydf.country[row_num]=="Ethiopia":
            region = "Africa"
        elif mydf.country[row_num]=="Cambodia" or mydf.country[row_num]=="Burma" or \
            mydf.country[row_num]=="Thailand" or mydf.country[row_num]=="Philippines" or \
            mydf.country[row_num]=="Nepal":
            region = "Asia"
        elif mydf.country[row_num]=="Guatemala" or mydf.country[row_num]=="Panama" or \
            mydf.country[row_num]=="Haiti":
            region = "Central America/ Caribbean"
        else:
            region = None

        #convert each datetime.timedelta to number of days
        if pd.notnull(mydf.funding_time[row_num]):
            days_diff = 1. + (mydf.funding_time[row_num].total_seconds() / (60.*60*24))
        else:
            days_diff = None
        if days_diff<1 or days_diff>30:
            #print "warning: invalid funding time for row " , row_num
            days_diff = None

        #dollars per day donated
        if pd.notnull(mydf.treat_cost[row_num]) and pd.notnull(days_diff):
            dollars_per_day = mydf.treat_cost[row_num] / days_diff
        else:
            #print "\nWarning: cannot calculate dollars_per_day for row " , row_num
            dollars_per_day = None

        region_list.append(region)
        dollars_per_day_list.append(dollars_per_day)
        days_diff_list.append(days_diff)

    mydf['region'] = region_list 
    mydf['dollars_per_day'] = dollars_per_day_list 
    mydf['funding_days'] = days_diff_list   

    
    #create indicator for day of week (monday, tuesday, etc.)
    week_day_names = [x.strftime('%A') for x in mydf['date_posted']]
    mydf['week_day_name_posted'] = week_day_names

    #create indicator for day of week (0 - 6; 0 is Monday)
    week_day_nums = [x.weekday() for x in mydf['date_posted']]
    mydf['week_day_num_posted'] = week_day_nums

    #indicator for weekend
    day_to_weekend = {
        'Monday': 0,
        'Tuesday': 0,
        'Wednesday': 0,
        'Thursday': 0,
        'Friday': 0,
        'Saturday': 1,
        'Sunday': 1
    }
    mydf['weekend_post'] = mydf['week_day_name_posted'].map(day_to_weekend)

    mydf.to_csv(towrite_path)


def read_data_csv():

    mydf = pd.read_csv(towrite_path)

    #want to make sure dates come in as dates
    mydf['date_posted'] = pd.to_datetime(mydf['date_posted'])
    mydf['date_funded'] = pd.to_datetime(mydf['date_funded'])

    mydf = mydf.drop(['Unnamed: 0'], axis=1) 
    
    return mydf


def convert_to_monthly(mydf):
    """Create a dataset containing monthly averages
    """

    #create time series with index indicating monthly period
    mydf.index=mydf['date_posted']
    #convert to having a monthly "period" index
    myts_monthly = mydf.to_period(freq='M') 
    #monthly averages in which each row represents 1 month
    mydf_M_avg = mydf.resample('M', how='mean').to_period(freq='M')
    mydf_M_avg = mydf_M_avg.drop(['profile_ID'], axis=1)
    #number of profiles posted each month 
    myts_M_count = mydf['profile_ID'].resample('M', how='count').to_period(freq='M')
    mydf_M_count = pd.DataFrame(data=myts_M_count, 
        index=myts_M_count.index, columns=['profiles_posted'])
    #combine to create a monthly time series dataframe
    monthly_df = pd.merge(
        pd.DataFrame(mydf_M_count),mydf_M_avg,right_index=True, left_index=True)
    
    return monthly_df


def summarize_data(mydf):

    print "\n************* Summary Stats ****************\n"

    # Data overview (means, stds, min, max of all numeric variables)   
    print mydf.describe()


    ## Data exploration ##

    #bar graph showing distribution of smile scores
    plt.hist(mydf['smile_scale'].dropna(), bins=11)
    plt.xlabel('Smile scale (0 - 1)')
    plt.ylabel('Number of Profiles')
    plt.savefig("01_SmileBars.pdf") #how to save to different location?
    plt.clf()

    #bar graph showing distribution of patient ages
    max_age = max(mydf['patient_age'].dropna())
    print "Max age:" , max_age
    min_age = min(mydf['patient_age'].dropna()) 
    print "Min age:" , min_age
    plt.hist(mydf['patient_age'].dropna(), bins=max_age)
    plt.xlabel('Patient Age')
    plt.ylabel('Number of Profiles')
    plt.savefig("01_AgeBars.pdf")
    plt.clf()

    #bar graph showing treatment cost
    mydata = mydf['treat_cost'].dropna()
    plt.hist(mydata, bins=20) #since max cost is $2000, each bin represents $100
    plt.xlabel('Treatment Cost (US dollars)')
    plt.ylabel('Number of Profiles')
    plt.savefig("01_CostBars.pdf")
    plt.clf()
    #display percentiles
    percentiles = [0,5] + range(10,100,10) + [95,99,100]
    print "\nTreatment Cost percentiles\n"
    for myp in percentiles:
        myq = myp/100.
        print "Percentile" , myp , ":" ,mydata.quantile(myq)

    #bar graph showing time till fully funded
    mydata = mydf['funding_days'].dropna()
    max_days =  mydata.max()
    plt.hist(mydata, bins=max_days)
    plt.xlabel('Time till fully funded (days)')
    plt.ylabel('Number of Profiles')
    plt.savefig("01_FundDaysBars.pdf")
    plt.clf()

    #time trends (monthly averages)
    monthly_df = convert_to_monthly(mydf)
    fig, axes = plt.subplots(3, 1)
    monthly_df[['treat_cost','dollars_per_day','text_length']].plot(
        figsize=(15,15),ax=axes[0])
    monthly_df[['profiles_posted','patient_age','funding_days']].plot(
        figsize=(15,15), ax=axes[1]) 
    monthly_df[['maleness','smile_scale']].plot(figsize=(15,15), ax=axes[2]) 
    plt.savefig("01_TimeTrends.pdf")
    plt.clf()


    ## Correlates with donation rate (visuals) ##

    #scatter plot of donation rate by treatment cost
    plt.scatter(mydf['treat_cost'], mydf['dollars_per_day'])
    plt.xlabel('Treatment Cost (US dollars)')
    plt.ylabel('Donations per Day (US dollars)')
    plt.xlim(0,2100) #there was just one $3000 treatment that gets chopped with this scale
    plt.ylim(0,2100)
    plt.savefig("02_Donation_by_Cost.pdf")
    plt.clf()
    #While most treatment costs are covered in first 3 days, there is still variation worth exploring
      #i.e., for the same treatment cost, why do donations per day differ?

    #donations by day of week (bars)
    #mean donation rate by day of the week
    d_by_w = mydf.groupby(['week_day_name_posted']).mean()
    #print d_by_w
    print type(d_by_w)
    d_by_w = d_by_w.sort_index(by=['week_day_num_posted'], ascending=False)
    print d_by_w
    d_by_w['dollars_per_day'].plot(kind='barh', color='b', alpha=0.7)
    plt.ylabel("Day of week posted")
    plt.xlabel("Average daily donation (US dollars)")
    #suggests people donate more during the work week
    plt.savefig("02_Donation_by_WeekDay.pdf")
    plt.clf()

    #Donations by smile index
    #mean donation rate by smile
    d_by_s = mydf.groupby(['smile_scale']).mean()
    print d_by_s
    print type(d_by_s)
    #d_by_s = d_by_s.sort_index()
    #print d_by_s
    d_by_s['dollars_per_day'].plot(kind='barh', color='b', alpha=0.7, fontsize='x-large')
    plt.ylabel("Smile scale (0 - 1)", fontsize='x-large')
    plt.xlabel("Average daily donation (US dollars)", fontsize='x-large')
    plt.savefig("02_Donation_by_Smile.pdf")
    plt.clf()

    #Avg donation by age-group and region


    ## Correlates with smile scale (visuals) ##
    

def run_regressions(mydf):

    print "\n************ Regression Results ************\n"

    #run a very simple regression to estimate effects of regressors on outcome
    results = smf.ols('dollars_per_day ~ \
                      C(week_day_name_posted) + day_posted + C(region) + maleness + \
                      treat_cost + patient_age:smile_scale + \
                        patient_age + smile_scale', data=mydf).fit()
    print results.summary()
    #smile scale is negative but lacks statistical signficance


    # model after dropping insignificant terms (backwards selection process)
    results = smf.ols('dollars_per_day ~ \
                      weekend_post + treat_cost + patient_age + smile_scale', data=mydf).fit()
    print results.summary()
    #smile scale is negative with p-val<.1

    
    # run with smile categories (do not treat as linear relationship)
    mydf = pd.read_csv(towrite_path)
    bins = [0, .45, .55, 1]
    smile_cat_names = ["negative","neutral","positive"]
    smile_dums = pd.get_dummies(pd.cut(mydf.smile_scale, bins, labels=smile_cat_names))
    mydf = pd.merge(mydf,smile_dums,left_index=True,right_index=True)
    results = smf.ols('dollars_per_day ~ \
                      treat_cost + patient_age + \
                      weekend_post + negative + positive', data=mydf).fit()
    print results.summary() 
    #being neutral is better than being positive, and maybe better than being negative


def prepare_data_for_RF(mydf):
    """Make a couple of data alterations to allow random forest to work.
    """

    #eliminate rows that have 1 or more missing values
    mydf = mydf.dropna(axis=0)
    #convert region to something numerical
    numeric_regions = {
    'Africa': 1,
        'Asia': 2,
        'Central America/ Caribbean': 3,
    }
    mydf['region_num'] = mydf['region'].map(numeric_regions) 

    return mydf


def run_random_forest(mydf):

    print "\n************ Random Forest Results ************\n"

    mydf = prepare_data_for_RF(mydf)   
    
    predictor_names = ['week_day_num_posted','day_posted','maleness','region_num', \
                      'treat_cost','patient_age','smile_scale']
    numfeat = len(predictor_names)
    Y = mydf.dollars_per_day #variable to predict
    X = mydf[predictor_names]
    
    #Build classifier using "best" random forest
    nfolds = 3 #number of folds to use for cross-validation
    #n_estimators is number of trees in forest
    #max_features is the number of features to consider when looking for best split
    parameters = {'n_estimators':[10,100,1000],  'max_features':[3,5,7]} # to try
    njobs = 1 #number of jobs to run in parallel
    rf_tune = grid_search.GridSearchCV(RandomForestRegressor(), parameters,
                             n_jobs = njobs, cv = nfolds)
    rf_opt = rf_tune.fit(X,Y)
    
    #Results of the grid search for optimal random forest parameters.
    print("Grid of scores:\n" + str(rf_opt.grid_scores_) + "\n")
    print("Best zero-one score: " + str(rf_opt.best_score_) + "\n")
    print("Optimal Model:\n" + str(rf_opt.best_estimator_) + "\n")
    #print "Parameters of random forest:\n " , rf_opt.get_params()

    #Now use the optimal model's parameters to run random forest
        #(I couldn't get feature importances directly from the GridSearchCV fit)
    crf = RandomForestRegressor(
        n_jobs=njobs, max_features=3, n_estimators=1000).fit(X,Y) 
    print "Parameters used in chosen RF model:\n " , crf.get_params()

    plotting_names = np.array(('Day','Date','Sex','Region','Cost','Age','Smile'))
    #print crf.feature_importances_
    indices = np.argsort(crf.feature_importances_)[::-1][:numfeat]
    plt.bar(xrange(numfeat), crf.feature_importances_[indices], \
        align='center', alpha=.5)
    plt.xticks(xrange(numfeat), plotting_names[indices], \
        rotation='horizontal', fontsize=20)
    plt.xlim([-1, numfeat])
    plt.ylabel('Feature importances', fontsize=24)
    plt.title('', fontsize=28)
    plt.savefig('03_feature_importance_v2.pdf');


def get_ATE(X,Y,donation_rate):

    ## Calculate probability of smiling (smile_scale>.5) for each patient (="Prob(T)")
        #use random forest to predict smile
    crf = RandomForestClassifier(
        n_jobs=1, max_features=3, n_estimators=1000).fit(X,Y) 

    #note: rf prob is fraction of trees in which patient is classified as smiling
    Y_predicted = np.array(crf.predict_proba(X))
    #print "Sample of predicted probabilities of smiling (vs. actual smile)\n"
    #for i in range(10):
    #    print Y[i] , "             " , "%.3f" % Y_predicted[i][1]
    #print "\n"

    ## Take average donation rate of population with weights = 1/Prob(T) 
    wt_smile = np.array([1./x[1] for x in Y_predicted]) #weights to apply
    wt_avg_smile = (donation_rate*wt_smile).sum() / wt_smile.sum()

    ## Take average donation rate of population with weights = 1/(1-Prob(T)
    wt_nosmile = np.array([1./x[0] for x in Y_predicted])
    wt_avg_nosmile = (donation_rate*wt_nosmile).sum() / wt_nosmile.sum()

    ## Average treatment effect (smiling versus not smiling) 
    ATE = wt_avg_smile - wt_avg_nosmile
   
    return ATE


def run_IPTW(mydf):

    print "\n************ IPTW Results ************\n"

    mydf = pd.read_csv(towrite_path)
    mydf = prepare_data_for_RF(mydf) 
    
    # create smile categories
    bins = [0, .5, 1]
    smile_cat_names = ["no_smile","yes_smile"]
    smile_dums = pd.get_dummies(pd.cut(mydf.smile_scale, bins, labels=smile_cat_names))
    #print smile_dums.ix[:20]
    mydf = pd.merge(mydf,smile_dums,left_index=True,right_index=True)

    Y = mydf.yes_smile
    #note: want to avoid including variables in X that have strong effect on smiling
        #and no or minimal effect on dollars_per_day
        #including such variables increases variability (and maybe adds to bias due to
        #positivity violations) for little gain in biase to control for confounding
    X = mydf[['week_day_num_posted','day_posted','maleness', \
                      'treat_cost','patient_age']]
    donation_rate = np.array(mydf.dollars_per_day) #for upcoming calc must be array

    ## Obtain point estimate
    ATE = get_ATE(X,Y,donation_rate) 
    print "Average treatment effect (dollars per day): " , ATE , "\n"

    ## Find std error of estimate via non-parametric bootstrap
    ATE_boots = []
    boot_count = 100 #number of bootstrap samples to take
    nrows = len(Y)
    Y.index = range(nrows)
    X.index = range(nrows)
    count = 0
    while count < boot_count:
        #get list of (nrows*.9) random numbers for row selection
        subset_size = int(nrows*.9)
        selected_rows = np.array(random.sample(xrange(nrows), subset_size)) 
        Ysample = Y.ix[selected_rows]
        Xsample = X.ix[selected_rows]
        Dsample = donation_rate[selected_rows]
        ATE_boot = get_ATE(Xsample, Ysample, Dsample)
        print "Result from 1 bootstrap sample: " , ATE_boot
        # Skip NaN results
        if np.isnan(ATE_boot): continue
        count += 1
        ATE_boots.append(ATE_boot)
    
    print "Bootstrap ATEs: " , sorted(ATE_boots)
    np.array(ATE_boots)    
    #display percentiles of bootstrap
    percentiles = [1,5,10,90,95,99]
    print "\nBootstrap confidence interval bounds (IPTW)\n"
    for myp in percentiles:
        print "Percentile" , myp , ":" , np.percentile( np.array(ATE_boots), myp)


###############################################################################

def main():


    ###### Obtain data and create CSV with relevant features ######

    ## Get spreadsheet info from google doc 
    spreadsheet_df = get_spreadsheet_info()

    ## Get data from photo and text 
        #if desired html files are already downloaded, 
         #set download to 0 to prevent re-downloading these files
    expanded_df = get_text_and_photo_data(mydf_main=spreadsheet_df, download=0)

    ## Create additional variables and write data to file
    write_data_csv(expanded_df)


    ###### Analysis using data from CSV file ######

    ## Read in csv and correct formating that was lost in transition
    mydf = read_data_csv()

    ## Produce summary stats and graphs
    summarize_data(mydf)

    ## Linear regression analysis
    run_regressions(mydf)

    ## Variable importance estimation via random forest
    run_random_forest(mydf)

    ## IPTW
    run_IPTW(mydf) 


main()



{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pprint\n",
      "import json\n",
      "from datetime import datetime\n",
      "from dateutil.parser import parse\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import defaultdict\n",
      "from itertools import izip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/pytz/__init__.py:35: UserWarning: Module dap was already imported from None, but /usr/lib/python2.7/dist-packages is being added to sys.path\n",
        "  from pkg_resources import resource_stream\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_df(FileName):\n",
      "    \"\"\"\n",
      "    Create data from from downloaded json file.\n",
      "    \"\"\"\n",
      "    #Download a dump of data about closed GitHub issues for the pandas project here:\n",
      "        #https://www.dropbox.com/s/pe6dqooznrfynii/closed.json\n",
      "    #Use the built-in json library to read this file into memory. Each element in\n",
      "    #the list contains information about a GitHub issue and all developer comments\n",
      "    #that were made on it in the 'comments' field.\n",
      "\n",
      "    myfile = json.load(open(FileName, \"r\"))\n",
      "    #check out the first 5 items\n",
      "    #pprint.pprint(myfile[:5])\n",
      " \n",
      "    #1) Make a DataFrame with one row per issue with the following columns extracted\n",
      "        #from the issue data: ntitle, created_at, labels, closed_at, user, id\n",
      "    mydf = pd.DataFrame(index=range(len(myfile)), \n",
      "        columns=['title', 'created_at', 'labels', 'closed_at', 'user', 'id'])\n",
      "    for counter, element in enumerate(myfile):\n",
      "        mydf.title[counter] = myfile[counter]['title'] \n",
      "        mydf.created_at[counter] = myfile[counter]['created_at'] \n",
      "        mydf.labels[counter] = myfile[counter]['labels'] \n",
      "        mydf.closed_at[counter] = myfile[counter]['closed_at'] \n",
      "        #Transform the user values to be simply the 'login' string, so that the user\n",
      "        #column contains only string usernames.\n",
      "        mydf.user[counter] = myfile[counter]['user']['login']\n",
      "        mydf.id[counter] = myfile[counter]['id'] \n",
      "    #print \"\\nSample from dataframe produced for item 1:\\n\" , mydf.ix[:5] , \"\\n\"\n",
      "\n",
      "    #2) Remove duplicate rows by id from the DataFrame you just created using the id\n",
      "        #column's duplicated method.\n",
      "    print \"Number of rows before dropping duplicates: \" , mydf.shape[0], \"\\n\"\n",
      "    dedup = mydf.drop_duplicates(['id'])\n",
      "    print \"Number of rows after dropping duplicates (item 2): \" , dedup.shape[0], \"\\n\"\n",
      "    #keep in mind that indices of dedup are not sequential from 0 to dedup.shape[0]\n",
      "\n",
      "\n",
      "    #3,4) Convert the created_at and closed_at columns from string to datetime type.\n",
      "        #current fomat looks like this: 2010-09-29T00:45:31Z\n",
      "    for counter in list(dedup.index):\n",
      "        dedup.ix[counter]['created_at'] = datetime.strptime(dedup.ix[counter]['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
      "        dedup.ix[counter]['closed_at'] = datetime.strptime(dedup.ix[counter]['closed_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
      "        #these methods also work\n",
      "            #dedup.ix[counter]['created_at'] = parse(dedup.ix[counter]['created_at'])\n",
      "            #dedup.ix[counter]['created_at'] = pd.to_datetime(dedup.ix[counter]['created_at'])\n",
      "    #print \"Sample from dataframe produced for item 4:\\n\" , dedup.ix[:5] , \"\\n\"\n",
      "\n",
      "    return dedup\n",
      "\n",
      "ts = create_df(FileName=\"closed.json\")\n",
      "print \"Time series (from items 0 - 4)\\n\" ,  ts.ix[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of rows before dropping duplicates:  2968 \n",
        "\n",
        "Number of rows after dropping duplicates (item 2):  2934 \n",
        "\n",
        "Time series (from items 0 - 4)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                               title           created_at  \\\n",
        "0  Enable element-wise comparison operations in D...  2010-09-29 00:45:31   \n",
        "2                              reindex_like function  2010-09-29 00:50:13   \n",
        "4                Binary operations on int DataMatrix  2010-09-29 00:50:52   \n",
        "\n",
        "  labels            closed_at  user      id  \n",
        "0     []  2011-02-19 23:13:48  wesm  337721  \n",
        "2     []  2010-12-17 02:57:33  wesm  337726  \n",
        "4     []  2011-01-01 23:50:12  wesm  337728  \n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#9) Create a helper 'labels' table from the issues data with two columns: id and\n",
      "#label. If an issue has 3 elements in its 'labels' value, add 3 rows to the\n",
      "#table. If an issue does not have any labels, place a single row with None as\n",
      "#the label (hint: construct a list of tuples, then make the DataFrame)."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileName=\"closed.json\"\n",
      "myfile = json.load(open(FileName, \"r\"))\n",
      "print \"Number of rows in original df: \" , len(myfile), \"\\n\"\n",
      "print \"Row 1: \\n\" , myfile[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of rows in original df:  2968 \n",
        "\n",
        "Row 1: \n",
        "{u'body': u're: pystatsmodels e-mail\\r\\n\\r\\n\\r\\nhi everyone,\\r\\n\\r\\njust getting started with pandas and i was wondering if someone could\\r\\nhelp me out.  do pandas.DataMatrix objects support per item comparison\\r\\noperations?\\r\\n\\r\\ni have a two data matrices, and i want to do something like this:\\r\\n\\r\\ndiv[div > 0.5 * price] = 0\\r\\n\\r\\nthis would work if div and price were numpy.ndarray objects.  any idea\\r\\nhow i would do something like this with pandas.DataMatrix objects?\\r\\n\\r\\nthanks,\\r\\nandy\\r\\n', u'events_url': u'https://api.github.com/repos/pydata/pandas/issues/1/events', u'title': u'Enable element-wise comparison operations in DataMatrix objects', u'url': u'https://api.github.com/repos/pydata/pandas/issues/1', u'labels_url': u'https://api.github.com/repos/pydata/pandas/issues/1/labels{/name}', u'created_at': u'2010-09-29T00:45:31Z', u'labels': [], u'comments_url': u'https://api.github.com/repos/pydata/pandas/issues/1/comments', u'html_url': u'https://github.com/pydata/pandas/issues/1', u'comments': [{u'text': u'implemented in git HEAD', u'updated': u'1298157227000', u'author': u'wesm', u'created': u'1298157227000'}], u'number': 1, u'updated_at': u'2013-04-26T21:25:39Z', u'assignee': None, u'state': u'closed', u'user': {u'following_url': u'https://api.github.com/users/wesm/following', u'gists_url': u'https://api.github.com/users/wesm/gists{/gist_id}', u'organizations_url': u'https://api.github.com/users/wesm/orgs', u'url': u'https://api.github.com/users/wesm', u'events_url': u'https://api.github.com/users/wesm/events{/privacy}', u'html_url': u'https://github.com/wesm', u'subscriptions_url': u'https://api.github.com/users/wesm/subscriptions', u'avatar_url': u'https://secure.gravatar.com/avatar/2c08a3eed709a9d1a2654cea45aa466f?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png', u'repos_url': u'https://api.github.com/users/wesm/repos', u'received_events_url': u'https://api.github.com/users/wesm/received_events', u'gravatar_id': u'2c08a3eed709a9d1a2654cea45aa466f', u'starred_url': u'https://api.github.com/users/wesm/starred{/owner}{/repo}', u'login': u'wesm', u'type': u'User', u'id': 329591, u'followers_url': u'https://api.github.com/users/wesm/followers'}, u'milestone': None, u'closed_at': u'2011-02-19T23:13:48Z', u'pull_request': {u'diff_url': None, u'html_url': None, u'patch_url': None}, u'id': 337721}\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileName=\"closed.json\"\n",
      "myfile = json.load(open(FileName, \"r\"))\n",
      "print \"Number of rows in original df: \" , len(myfile), \"\\n\"\n",
      "#print \"First row: \" , myfile[0]\n",
      "\n",
      "mytups = []\n",
      "for counter, element in enumerate(myfile):\n",
      "    if len(myfile[counter]['labels'])==0:\n",
      "        mytups.append((myfile[counter]['id'],\"None\"))\n",
      "    else:\n",
      "        nlab = 0\n",
      "        for label in myfile[counter]['labels']:\n",
      "            mytups.append(( myfile[counter]['id'], myfile[counter]['labels'][nlab]['name'] ))\n",
      "            nlab += 1\n",
      "#print mytups\n",
      "\n",
      "#build data frame expanded so each comment gets a row\n",
      "mydf = pd.DataFrame(index=range(len(mytups)), columns=['id','label'])     \n",
      "for row, tup in enumerate(mytups):\n",
      "    mydf.ix[row]['id'] = tup[0]\n",
      "    mydf.ix[row]['label'] = tup[1]\n",
      "\n",
      "#get rid of duplicated rows \n",
      "    #I assume we should though assignment does not specify here\n",
      "label_df = mydf.drop_duplicates(['id','label'])\n",
      "print \"\\nSample from dataframe containing labels-id mapping (item 9): \\n\" , label_df[:30]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of rows in original df:  2968 \n",
        "\n",
        "\n",
        "Sample from dataframe containing labels-id mapping (item 9): \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        id        label\n",
        "0   337721         None\n",
        "2   337726         None\n",
        "4   337728         None\n",
        "6   337730         None\n",
        "8   337736         None\n",
        "10  337994         None\n",
        "12  338909         None\n",
        "14  339355          Bug\n",
        "16  341577         None\n",
        "18  341581         None\n",
        "20  341583         None\n",
        "22  344725         None\n",
        "24  352369  Enhancement\n",
        "26  356064         None\n",
        "28  358943         None\n",
        "30  358947         None\n",
        "32  358950  Enhancement\n",
        "34  358952          Bug\n",
        "36  376890         None\n",
        "38  428564         None\n",
        "40  428981         None\n",
        "42  474160         None\n",
        "44  488026         None\n",
        "46  491181         None\n",
        "48  491185         None\n",
        "50  503940         None\n",
        "52  504715         None\n",
        "54  518963         None\n",
        "56  519475         None\n",
        "58  519516         None\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#10) Now, join the issues data with the labels helper table (pandas.merge). \n",
      "\n",
      "df10 = pd.merge(ts, label_df, on='id', how='outer')\n",
      "\n",
      "\n",
      "#Add a column to this table containing the number of days (as a floating point\n",
      "#number) it took to close each issue.\n",
      "\n",
      "time_open = np.array(df10[\"closed_at\"])-np.array(df10[\"created_at\"])\n",
      "#print type(time_open[5])\n",
      "days_open = []\n",
      "for i in range(len(time_open)):\n",
      "    #convert each datetime.timedelta to number of days\n",
      "    days_diff = time_open[i].total_seconds() / (60.*60*24)\n",
      "    days_open.append(days_diff)\n",
      "df10['days_open'] = days_open\n",
      "\n",
      "print df10[:30][['id','created_at','closed_at','days_open','label']]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        id           created_at            closed_at   days_open        label\n",
        "0   337721  2010-09-29 00:45:31  2011-02-19 23:13:48  143.936308         None\n",
        "1   337726  2010-09-29 00:50:13  2010-12-17 02:57:33   79.088426         None\n",
        "2   337728  2010-09-29 00:50:52  2011-01-01 23:50:12   94.957870         None\n",
        "3   337730  2010-09-29 00:51:27  2010-12-11 06:14:32   73.224363         None\n",
        "4   337736  2010-09-29 00:57:00  2010-12-17 02:46:34   79.076088         None\n",
        "5   337994  2010-09-29 05:30:56  2010-12-11 06:05:26   73.023958         None\n",
        "6   338909  2010-09-29 15:41:55  2010-12-11 06:12:00   72.604225         None\n",
        "7   339355  2010-09-29 19:45:47  2011-06-23 04:50:05  266.377986          Bug\n",
        "8   341577  2010-09-30 22:29:36  2011-05-18 02:43:33  229.176354         None\n",
        "9   341581  2010-09-30 22:33:14  2011-06-23 19:38:51  265.878900         None\n",
        "10  341583  2010-09-30 22:34:26  2011-02-19 23:46:00  142.049699         None\n",
        "11  344725  2010-10-03 17:20:41  2010-12-11 22:53:14   69.230937         None\n",
        "12  352369  2010-10-07 23:42:34  2012-12-12 21:35:57  796.912072  Enhancement\n",
        "13  356064  2010-10-11 03:19:39  2011-02-19 16:13:00  131.537049         None\n",
        "14  358943  2010-10-12 16:10:48  2010-12-11 06:19:58   59.589699         None\n",
        "15  358947  2010-10-12 16:13:04  2011-06-23 04:46:14  253.523032         None\n",
        "16  358950  2010-10-12 16:13:55  2011-07-08 22:15:57  269.251412  Enhancement\n",
        "17  358952  2010-10-12 16:15:10  2011-09-25 05:17:23  347.543206          Bug\n",
        "18  376890  2010-10-22 17:59:31  2011-01-11 03:37:14   80.401192         None\n",
        "19  428564  2010-11-19 14:50:11  2010-12-17 02:45:20   27.496632         None\n",
        "20  428981  2010-11-19 18:41:36  2010-11-19 18:42:08    0.000370         None\n",
        "21  474160  2010-12-14 16:50:11  2010-12-15 04:07:42    0.470498         None\n",
        "22  488026  2010-12-22 10:37:40  2011-01-01 23:16:26   10.526921         None\n",
        "23  491181  2010-12-24 07:27:52  2010-12-24 16:27:12    0.374537         None\n",
        "24  491185  2010-12-24 07:32:39  2010-12-24 21:28:59    0.580787         None\n",
        "25  503940  2011-01-03 17:15:19  2011-01-03 23:23:47    0.255880         None\n",
        "26  504715  2011-01-03 23:29:47  2011-02-20 00:53:44   47.058299         None\n",
        "27  518963  2011-01-11 04:14:15  2011-01-29 19:13:49   18.624699         None\n",
        "28  519475  2011-01-11 12:15:39  2011-06-23 04:47:01  162.688449         None\n",
        "29  519516  2011-01-11 12:31:34  2011-06-14 14:17:06  154.073287         None\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#11) Compute a table containing the average time to close for each label\n",
      "#type. Now make a plot comparing mean time to close by month for Enhancement\n",
      "#versus Bug issue types.\n",
      "\n",
      "#create defaultdict with desired info\n",
      "\n",
      "grpd_data = defaultdict(list)\n",
      "for row_num, row in enumerate(df10.iterrows()):\n",
      "    grpd_data[(row[1]['label'], row[1]['created_at'].month)].append( row[1]['days_open'] )\n",
      "#test\n",
      "#for (label, date), value in grpd_data.iteritems():\n",
      "#    print label, date, sum(value)/len(value)\n",
      "    \n",
      "#build data frame \n",
      "\n",
      "mydf11 = pd.DataFrame(index=range(len(grpd_data)), columns=['month','label','mean_days_open'])     \n",
      "for row, ((label, date), value) in enumerate(grpd_data.iteritems()):\n",
      "    mydf11.ix[row]['month'] = date\n",
      "    mydf11.ix[row]['label'] = label\n",
      "    mydf11.ix[row]['mean_days_open'] = sum(value)/len(value)\n",
      "print mydf11[:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Testing 10 7.92694444444\n",
        "None 6 2.2202851776\n",
        "prio-low 3 27.5851099537\n",
        "Refactor 4 13.9768055556\n",
        "Build problem 4 2.71414930556\n",
        "Docs 9 9.47016203704\n",
        "Enhancement 8 75.1801022377\n",
        "Usage 4 0.801450617284\n",
        "timeseries 4 7.69859800312\n",
        "Data IO 10 72.8331983025\n",
        "Visualization 6 3.72711279461\n",
        "Enhancement 5 36.5538960954\n",
        "groupby 3 5.49799933862\n",
        "timeseries 11 27.9329541122\n",
        "Visualization 11 10.6445688657\n",
        "Bug 5 6.20388599537\n",
        "prio-high 1 9.13440104167\n",
        "Performance 3 1.28388599537\n",
        "Testing 4 8.72218253968\n",
        "Testing 9 40.5636689815\n",
        "None 11 4.29421759259\n",
        "Indexing 5 43.4906712963\n",
        "Dtypes 3 1.35506076389\n",
        "Data IO 4 12.9422646605\n",
        "Docs 4 4.80484953704\n",
        "Enhancement 11 43.3752029597\n",
        "unicode 7 10.9795486111\n",
        "timeseries 1 40.1276070602\n",
        "Data IO 9 30.8231469907\n",
        "Visualization 1 30.8082939815\n",
        "groupby 6 2.42170524691\n",
        "prio-low 10 48.2828356481\n",
        "Bug 8 18.4960653935\n",
        "prio-high 2 16.4072732991\n",
        "Good as first PR 3 0.203125\n",
        "Testing 3 0.821763117284\n",
        "None 1 10.7884268094\n",
        "None 12 8.36120817772\n",
        "Indexing 10 45.7044282407\n",
        "Refactor 2 14.1498109568\n",
        "prio-medium 2 38.962697586\n",
        "Data IO 3 32.2421990741\n",
        "Docs 7 63.7993287037\n",
        "timeseries 2 35.6951362179\n",
        "Visualization 12 19.4531111111\n",
        "groupby 9 144.679236111\n",
        "missing-data 3 98.4827083333\n",
        "Community 1 45.5903634259\n",
        "Build problem 12 13.3369957011\n",
        "Reshaping 3 5.92097222222\n",
        "Output-Formatting 2 38.0959172454\n",
        "None 2 5.73653302787\n",
        "Regression 4 2.09400958995\n",
        "Refactor 8 6.53263888889\n",
        "prio-medium 8 15.2084490741\n",
        "Multithreading 11 27.8291782407\n",
        "Community 5 79.6397646605\n",
        "Docs 2 33.1285393519\n",
        "Enhancement 1 54.824094624\n",
        "Ideas 4 3.89131944444\n",
        "Regression 3 1.54615740741\n",
        "Dtypes 1 39.8169560185\n",
        "Bug 1 19.0760872543\n",
        "groupby 12 1.06974537037\n",
        "Can't Repro 2 35.7862268519\n",
        "Output-Formatting 12 13.0577835648\n",
        "Build problem 9 3.67735146605\n",
        "Bug 11 8.00767539983\n",
        "Performance 8 5.68776041667\n",
        "None 7 8.2185587963\n",
        "Indexing 1 39.4131365741\n",
        "Refactor 5 2.69888888889\n",
        "Docs 8 16.977093254\n",
        "timeseries 5 9.55965084877\n",
        "Build problem 2 10.5508541667\n",
        "Visualization 5 7.41073688272\n",
        "Enhancement 4 17.0473051075\n",
        "groupby 2 99.9685127315\n",
        "timeseries 8 23.3195095486\n",
        "missing-data 4 6.89841435185\n",
        "Visualization 10 10.1260223765\n",
        "Build problem 3 52.3172962963\n",
        "unicode 10 173.975902778\n",
        "Output-Formatting 9 16.3450491898\n",
        "Testing 7 160.258344907\n",
        "Testing 8 11.9406828704\n",
        "None 8 10.8932368827\n",
        "Refactor 6 0.745306712963\n",
        "Dtypes 2 33.4411747685\n",
        "Data IO 7 13.9943460648\n",
        "Docs 11 2.95262731481\n",
        "Enhancement 10 57.2133318237\n",
        "timeseries 6 1.43718381734\n",
        "Community 3 163.99720679\n",
        "Can't Repro 12 0.0346064814815\n",
        "Enhancement 7 62.3405429602\n",
        "Ideas 10 7.64614583333\n",
        "groupby 4 11.4585034722\n",
        "Bug 7 10.768493266\n",
        "prio-high 3 21.1241315158\n",
        "Testing 2 11.1861111111\n",
        "Can't Repro 10 162.828113426\n",
        "Build problem 11 8.3986882716\n",
        "Performance 9 7.13635416667\n",
        "Dtypes 4 0.381064814815\n",
        "Indexing 11 40.5185300926\n",
        "Performance 2 6.551875\n",
        "prio-medium 1 108.310127315\n",
        "Community 9 17.2483773148\n",
        "Note To Selves 3 36.2736689815\n",
        "Docs 6 5.00899016204\n",
        "timeseries 3 28.4195407197\n",
        "Build problem 1 17.4725655864\n",
        "Stats 7 0.998645833333\n",
        "Visualization 3 41.4619824735\n",
        "groupby 8 227.192060185\n",
        "prio-low 12 1.55635416667\n",
        "Bug 10 17.4286720008\n",
        "prio-high 4 9.70215994268\n",
        "Data IO 8 14.9889236111\n",
        "Output-Formatting 3 12.6724768519\n",
        "Testing 1 14.1428935185\n",
        "None 3 6.04469445709\n",
        "Bug 4 10.7139050647\n",
        "Refactor 9 6.25777777778\n",
        "Docs 12 42.1790149177\n",
        "Indexing 8 30.9645736883\n",
        "prio-medium 4 9.93271604938\n",
        "Data IO 1 36.4249390432\n",
        "Docs 1 26.7931321225\n",
        "Ideas 3 20.0214853395\n",
        "Reshaping 4 0.0981018518519\n",
        "timeseries 12 63.8862566138\n",
        "Stats 4 4.16866898148\n",
        "Build problem 7 42.1662673611\n",
        "groupby 5 2.43158130787\n",
        "Build problem 10 47.9878240741\n",
        "Good as first PR 4 0.0168865740741\n",
        "Testing 12 7.21213831019\n",
        "None 4 3.50256388171\n",
        "Indexing 2 30.4135416667\n",
        "Reshaping 8 13.9943518519\n",
        "Performance 10 151.199236111\n",
        "Refactor 10 5.0478279321\n",
        "Community 7 41.6731365741\n",
        "Visualization 4 1.79268429487\n",
        "Enhancement 3 43.4826586329\n",
        "groupby 1 60.3012731481\n",
        "timeseries 9 55.8732272377\n",
        "Can't Repro 8 227.192060185\n",
        "Visualization 9 2.75324652778\n",
        "Bug 3 9.0505508977\n",
        "unicode 11 80.9022453704\n",
        "Community 10 51.727492284\n",
        "Output-Formatting 10 18.4139544753\n",
        "Testing 6 2.8513343254\n",
        "Testing 11 20.8335972222\n",
        "None 9 28.1635469771\n",
        "Build problem 6 14.3929571759\n",
        "Indexing 7 5.73434027778\n",
        "Refactor 7 75.4861689815\n",
        "groupby 7 8.07756481481\n",
        "Data IO 6 24.9256674383\n",
        "Docs 10 32.1106764403\n",
        "Enhancement 9 71.0569791667\n",
        "timeseries 7 25.386309651\n",
        "Data IO 11 10.7761381173\n",
        "Visualization 7 5.75322337963\n",
        "Enhancement 6 24.2490916005\n",
        "Ideas 9 37.6470138889\n",
        "unicode 1 38.5978935185\n",
        "timeseries 10 79.3332976466\n",
        "missing-data 6 0.0774305555556\n",
        "API 3 5.60109953704\n",
        "Bug 6 5.94352421918\n",
        "Output-Formatting 7 125.515474537\n",
        "Testing 5 8.80451822917\n",
        "Multithreading 12 23.9722916667\n",
        "Performance 11 2.09579861111\n",
        "None 10 21.3948199762\n",
        "Reshaping 11 7.38478587963\n",
        "Indexing 4 0.812699331276\n",
        "Performance 1 4.82476273148\n",
        "Data IO 5 56.8408382937\n",
        "Docs 5 11.8525400641\n",
        "Enhancement 12 31.2201806733\n",
        "prio-low 2 6.06122685185\n",
        "Visualization 2 34.0116388889\n",
        "prio-low 4 0.813321759259\n",
        "Ideas 12 50.0521122685\n",
        "unicode 2 27.6129861111\n",
        "Bug 9 20.2271325639\n",
        "prio-high 5 15.2330787037\n",
        "Can't Repro 4 5.2015162037\n",
        "Output-Formatting 4 1.31028356481\n",
        "Build problem 5 5.50119675926\n",
        "Indexing 9 76.492962963\n",
        "prio-medium 3 48.1793735532\n",
        "Community 11 9.57677083333\n",
        "Stats 10 59.4519020062\n",
        "Multithreading 1 75.2029050926\n",
        "API 4 0.891678240741\n",
        "prio-low 1 0.16755787037\n",
        "Stats 5 202.910798611\n",
        "Data IO 12 12.6091013072\n",
        "groupby 10 223.645092593\n",
        "Bug 12 15.5008023904\n",
        "prio-high 6 0.414618055556\n",
        "Output-Formatting 1 46.8966435185\n",
        "None 5 3.80473472222\n",
        "Data IO 2 27.0660069444\n",
        "Indexing 3 2.25738811728\n",
        "Refactor 11 13.3458564815\n",
        "Performance 4 2.22569155093\n",
        "unicode 4 5.67035108025\n",
        "Community 4 79.3554398148\n",
        "Docs 3 38.6262742003\n",
        "Enhancement 2 53.3081552212\n",
        "Ideas 5 4.12174768519\n",
        "unicode 12 10.4681597222\n",
        "missing-data 10 54.3666898148\n",
        "Bug 2 22.0592343074\n",
        "unicode 8 8.4051099537\n",
        "Output-Formatting 11 4.19269386574\n",
        "Build problem 8 15.3940162037\n",
        "  month          label mean_days_open\n",
        "0    10        Testing       7.926944\n",
        "1     6           None       2.220285\n",
        "2     3       prio-low       27.58511\n",
        "3     4       Refactor       13.97681\n",
        "4     4  Build problem       2.714149\n",
        "5     9           Docs       9.470162\n",
        "6     8    Enhancement        75.1801\n",
        "7     4          Usage      0.8014506\n",
        "8     4     timeseries       7.698598\n",
        "9    10        Data IO        72.8332"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#select just label=\"Enhancement\"\n",
      "myenhance = mydf11[mydf11.label==\"Enhancement\"]\n",
      "\n",
      "#select just label=\"bug\"\n",
      "mybugs = mydf11[mydf11.label==\"Bug\"]\n",
      "print mybugs\n",
      "#myenhance.plot(x = myenhance['month'], y=myenhance['mean_days_open'], label='bugs',\n",
      " #       title=\"Average time between creation and clusure for bugs vs. enhancements\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    month label mean_days_open\n",
        "15      5   Bug       6.203886\n",
        "32      8   Bug       18.49607\n",
        "62      1   Bug       19.07609\n",
        "67     11   Bug       8.007675\n",
        "98      7   Bug       10.76849\n",
        "117    10   Bug       17.42867\n",
        "123     4   Bug       10.71391\n",
        "151     3   Bug       9.050551\n",
        "174     6   Bug       5.943524\n",
        "191     9   Bug       20.22713\n",
        "206    12   Bug        15.5008\n",
        "221     2   Bug       22.05923\n"
       ]
      }
     ],
     "prompt_number": 62
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pprint\n",
      "import json\n",
      "from datetime import datetime\n",
      "from dateutil.parser import parse\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import defaultdict\n",
      "from itertools import izip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/pytz/__init__.py:35: UserWarning: Module dap was already imported from None, but /usr/lib/python2.7/dist-packages is being added to sys.path\n",
        "  from pkg_resources import resource_stream\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_df(FileName):\n",
      "    \"\"\"\n",
      "    Create data from from downloaded json file.\n",
      "    \"\"\"\n",
      "    #Download a dump of data about closed GitHub issues for the pandas project here:\n",
      "        #https://www.dropbox.com/s/pe6dqooznrfynii/closed.json\n",
      "    #Use the built-in json library to read this file into memory. Each element in\n",
      "    #the list contains information about a GitHub issue and all developer comments\n",
      "    #that were made on it in the 'comments' field.\n",
      "\n",
      "    myfile = json.load(open(FileName, \"r\"))\n",
      "    #check out the first 5 items\n",
      "    #pprint.pprint(myfile[:5])\n",
      " \n",
      "    #1) Make a DataFrame with one row per issue with the following columns extracted\n",
      "        #from the issue data: ntitle, created_at, labels, closed_at, user, id\n",
      "    mydf = pd.DataFrame(index=range(len(myfile)), \n",
      "        columns=['title', 'created_at', 'labels', 'closed_at', 'user', 'id'])\n",
      "    for counter, element in enumerate(myfile):\n",
      "        mydf.title[counter] = myfile[counter]['title'] \n",
      "        mydf.created_at[counter] = myfile[counter]['created_at'] \n",
      "        mydf.labels[counter] = myfile[counter]['labels'] \n",
      "        mydf.closed_at[counter] = myfile[counter]['closed_at'] \n",
      "        #Transform the user values to be simply the 'login' string, so that the user\n",
      "        #column contains only string usernames.\n",
      "        mydf.user[counter] = myfile[counter]['user']['login']\n",
      "        mydf.id[counter] = myfile[counter]['id'] \n",
      "    #print \"\\nSample from dataframe produced for item 1:\\n\" , mydf.ix[:5] , \"\\n\"\n",
      "\n",
      "    #2) Remove duplicate rows by id from the DataFrame you just created using the id\n",
      "        #column's duplicated method.\n",
      "    print \"Number of rows before dropping duplicates: \" , mydf.shape[0], \"\\n\"\n",
      "    dedup = mydf.drop_duplicates(['id'])\n",
      "    print \"Number of rows after dropping duplicates (item 2): \" , dedup.shape[0], \"\\n\"\n",
      "    #keep in mind that indices of dedup are not sequential from 0 to dedup.shape[0]\n",
      "\n",
      "\n",
      "    #3,4) Convert the created_at and closed_at columns from string to datetime type.\n",
      "        #current fomat looks like this: 2010-09-29T00:45:31Z\n",
      "    for counter in list(dedup.index):\n",
      "        dedup.ix[counter]['created_at'] = datetime.strptime(dedup.ix[counter]['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
      "        dedup.ix[counter]['closed_at'] = datetime.strptime(dedup.ix[counter]['closed_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
      "        #these methods also work\n",
      "            #dedup.ix[counter]['created_at'] = parse(dedup.ix[counter]['created_at'])\n",
      "            #dedup.ix[counter]['created_at'] = pd.to_datetime(dedup.ix[counter]['created_at'])\n",
      "    #print \"Sample from dataframe produced for item 4:\\n\" , dedup.ix[:5] , \"\\n\"\n",
      "\n",
      "    return dedup\n",
      "\n",
      "ts = create_df(FileName=\"closed.json\")\n",
      "print \"Time series (from items 0 - 4)\\n\" ,  ts.ix[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of rows before dropping duplicates:  2968 \n",
        "\n",
        "Number of rows after dropping duplicates (item 2):  2934 \n",
        "\n",
        "Time series (from items 0 - 4)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                               title           created_at  \\\n",
        "0  Enable element-wise comparison operations in D...  2010-09-29 00:45:31   \n",
        "2                              reindex_like function  2010-09-29 00:50:13   \n",
        "4                Binary operations on int DataMatrix  2010-09-29 00:50:52   \n",
        "\n",
        "  labels            closed_at  user      id  \n",
        "0     []  2011-02-19 23:13:48  wesm  337721  \n",
        "2     []  2010-12-17 02:57:33  wesm  337726  \n",
        "4     []  2011-01-01 23:50:12  wesm  337728  \n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#9) Create a helper 'labels' table from the issues data with two columns: id and\n",
      "#label. If an issue has 3 elements in its 'labels' value, add 3 rows to the\n",
      "#table. If an issue does not have any labels, place a single row with None as\n",
      "#the label (hint: construct a list of tuples, then make the DataFrame)."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileName=\"closed.json\"\n",
      "myfile = json.load(open(FileName, \"r\"))\n",
      "print \"Number of rows in original df: \" , len(myfile), \"\\n\"\n",
      "print \"Row 1: \\n\" , myfile[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of rows in original df:  2968 \n",
        "\n",
        "Row 1: \n",
        "{u'body': u're: pystatsmodels e-mail\\r\\n\\r\\n\\r\\nhi everyone,\\r\\n\\r\\njust getting started with pandas and i was wondering if someone could\\r\\nhelp me out.  do pandas.DataMatrix objects support per item comparison\\r\\noperations?\\r\\n\\r\\ni have a two data matrices, and i want to do something like this:\\r\\n\\r\\ndiv[div > 0.5 * price] = 0\\r\\n\\r\\nthis would work if div and price were numpy.ndarray objects.  any idea\\r\\nhow i would do something like this with pandas.DataMatrix objects?\\r\\n\\r\\nthanks,\\r\\nandy\\r\\n', u'events_url': u'https://api.github.com/repos/pydata/pandas/issues/1/events', u'title': u'Enable element-wise comparison operations in DataMatrix objects', u'url': u'https://api.github.com/repos/pydata/pandas/issues/1', u'labels_url': u'https://api.github.com/repos/pydata/pandas/issues/1/labels{/name}', u'created_at': u'2010-09-29T00:45:31Z', u'labels': [], u'comments_url': u'https://api.github.com/repos/pydata/pandas/issues/1/comments', u'html_url': u'https://github.com/pydata/pandas/issues/1', u'comments': [{u'text': u'implemented in git HEAD', u'updated': u'1298157227000', u'author': u'wesm', u'created': u'1298157227000'}], u'number': 1, u'updated_at': u'2013-04-26T21:25:39Z', u'assignee': None, u'state': u'closed', u'user': {u'following_url': u'https://api.github.com/users/wesm/following', u'gists_url': u'https://api.github.com/users/wesm/gists{/gist_id}', u'organizations_url': u'https://api.github.com/users/wesm/orgs', u'url': u'https://api.github.com/users/wesm', u'events_url': u'https://api.github.com/users/wesm/events{/privacy}', u'html_url': u'https://github.com/wesm', u'subscriptions_url': u'https://api.github.com/users/wesm/subscriptions', u'avatar_url': u'https://secure.gravatar.com/avatar/2c08a3eed709a9d1a2654cea45aa466f?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png', u'repos_url': u'https://api.github.com/users/wesm/repos', u'received_events_url': u'https://api.github.com/users/wesm/received_events', u'gravatar_id': u'2c08a3eed709a9d1a2654cea45aa466f', u'starred_url': u'https://api.github.com/users/wesm/starred{/owner}{/repo}', u'login': u'wesm', u'type': u'User', u'id': 329591, u'followers_url': u'https://api.github.com/users/wesm/followers'}, u'milestone': None, u'closed_at': u'2011-02-19T23:13:48Z', u'pull_request': {u'diff_url': None, u'html_url': None, u'patch_url': None}, u'id': 337721}\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileName=\"closed.json\"\n",
      "myfile = json.load(open(FileName, \"r\"))\n",
      "print \"Number of rows in original df: \" , len(myfile), \"\\n\"\n",
      "#print \"First row: \" , myfile[0]\n",
      "\n",
      "mytups = []\n",
      "for counter, element in enumerate(myfile):\n",
      "    if len(myfile[counter]['labels'])==0:\n",
      "        mytups.append((myfile[counter]['id'],\"None\"))\n",
      "    else:\n",
      "        nlab = 0\n",
      "        for label in myfile[counter]['labels']:\n",
      "            mytups.append(( myfile[counter]['id'], myfile[counter]['labels'][nlab]['name'] ))\n",
      "            nlab += 1\n",
      "#print mytups\n",
      "\n",
      "#build data frame expanded so each comment gets a row\n",
      "mydf = pd.DataFrame(index=range(len(mytups)), columns=['id','label'])     \n",
      "for row, tup in enumerate(mytups):\n",
      "    mydf.ix[row]['id'] = tup[0]\n",
      "    mydf.ix[row]['label'] = tup[1]\n",
      "\n",
      "#get rid of duplicated rows \n",
      "    #I assume we should though assignment does not specify here\n",
      "label_df = mydf.drop_duplicates(['id','label'])\n",
      "print \"\\nSample from dataframe containing labels-id mapping (item 9): \\n\" , label_df[:30]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of rows in original df:  2968 \n",
        "\n",
        "\n",
        "Sample from dataframe containing labels-id mapping (item 9): \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        id        label\n",
        "0   337721         None\n",
        "2   337726         None\n",
        "4   337728         None\n",
        "6   337730         None\n",
        "8   337736         None\n",
        "10  337994         None\n",
        "12  338909         None\n",
        "14  339355          Bug\n",
        "16  341577         None\n",
        "18  341581         None\n",
        "20  341583         None\n",
        "22  344725         None\n",
        "24  352369  Enhancement\n",
        "26  356064         None\n",
        "28  358943         None\n",
        "30  358947         None\n",
        "32  358950  Enhancement\n",
        "34  358952          Bug\n",
        "36  376890         None\n",
        "38  428564         None\n",
        "40  428981         None\n",
        "42  474160         None\n",
        "44  488026         None\n",
        "46  491181         None\n",
        "48  491185         None\n",
        "50  503940         None\n",
        "52  504715         None\n",
        "54  518963         None\n",
        "56  519475         None\n",
        "58  519516         None\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#10) Now, join the issues data with the labels helper table (pandas.merge). \n",
      "\n",
      "df10 = pd.merge(ts, label_df, on='id', how='outer')\n",
      "\n",
      "#Add a column to this table containing the number of days (as a floating point\n",
      "#number) it took to close each issue.\n",
      "\n",
      "time_open = np.array(df10[\"closed_at\"])-np.array(df10[\"created_at\"])\n",
      "#print type(time_open[5])\n",
      "days_open = []\n",
      "for i in range(len(time_open)):\n",
      "    #convert each datetime.timedelta to number of days\n",
      "    days_diff = time_open[i].total_seconds() / (60.*60*24)\n",
      "    days_open.append(days_diff)\n",
      "df10['days_open'] = days_open\n",
      "\n",
      "print df10[:30][['id','created_at','closed_at','days_open','label']]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        id           created_at            closed_at   days_open        label\n",
        "0   337721  2010-09-29 00:45:31  2011-02-19 23:13:48  143.936308         None\n",
        "1   337726  2010-09-29 00:50:13  2010-12-17 02:57:33   79.088426         None\n",
        "2   337728  2010-09-29 00:50:52  2011-01-01 23:50:12   94.957870         None\n",
        "3   337730  2010-09-29 00:51:27  2010-12-11 06:14:32   73.224363         None\n",
        "4   337736  2010-09-29 00:57:00  2010-12-17 02:46:34   79.076088         None\n",
        "5   337994  2010-09-29 05:30:56  2010-12-11 06:05:26   73.023958         None\n",
        "6   338909  2010-09-29 15:41:55  2010-12-11 06:12:00   72.604225         None\n",
        "7   339355  2010-09-29 19:45:47  2011-06-23 04:50:05  266.377986          Bug\n",
        "8   341577  2010-09-30 22:29:36  2011-05-18 02:43:33  229.176354         None\n",
        "9   341581  2010-09-30 22:33:14  2011-06-23 19:38:51  265.878900         None\n",
        "10  341583  2010-09-30 22:34:26  2011-02-19 23:46:00  142.049699         None\n",
        "11  344725  2010-10-03 17:20:41  2010-12-11 22:53:14   69.230937         None\n",
        "12  352369  2010-10-07 23:42:34  2012-12-12 21:35:57  796.912072  Enhancement\n",
        "13  356064  2010-10-11 03:19:39  2011-02-19 16:13:00  131.537049         None\n",
        "14  358943  2010-10-12 16:10:48  2010-12-11 06:19:58   59.589699         None\n",
        "15  358947  2010-10-12 16:13:04  2011-06-23 04:46:14  253.523032         None\n",
        "16  358950  2010-10-12 16:13:55  2011-07-08 22:15:57  269.251412  Enhancement\n",
        "17  358952  2010-10-12 16:15:10  2011-09-25 05:17:23  347.543206          Bug\n",
        "18  376890  2010-10-22 17:59:31  2011-01-11 03:37:14   80.401192         None\n",
        "19  428564  2010-11-19 14:50:11  2010-12-17 02:45:20   27.496632         None\n",
        "20  428981  2010-11-19 18:41:36  2010-11-19 18:42:08    0.000370         None\n",
        "21  474160  2010-12-14 16:50:11  2010-12-15 04:07:42    0.470498         None\n",
        "22  488026  2010-12-22 10:37:40  2011-01-01 23:16:26   10.526921         None\n",
        "23  491181  2010-12-24 07:27:52  2010-12-24 16:27:12    0.374537         None\n",
        "24  491185  2010-12-24 07:32:39  2010-12-24 21:28:59    0.580787         None\n",
        "25  503940  2011-01-03 17:15:19  2011-01-03 23:23:47    0.255880         None\n",
        "26  504715  2011-01-03 23:29:47  2011-02-20 00:53:44   47.058299         None\n",
        "27  518963  2011-01-11 04:14:15  2011-01-29 19:13:49   18.624699         None\n",
        "28  519475  2011-01-11 12:15:39  2011-06-23 04:47:01  162.688449         None\n",
        "29  519516  2011-01-11 12:31:34  2011-06-14 14:17:06  154.073287         None\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#11) Compute a table containing the average time to close for each label\n",
      "#type. Now make a plot comparing mean time to close by month for Enhancement\n",
      "#versus Bug issue types.\n",
      "\n",
      "df10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}